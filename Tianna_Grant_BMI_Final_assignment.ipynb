{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of Neural Network with numpy: \n",
    "\n",
    "INTRODUCTION\n",
    "A artifical neural network(ANN) consists of layers containing nodes. Each node has inputs, weights and a bias. The output of a node is the sum of every input * its weight plus the bias value. In this assignment, I utilized a constructed neural network which outputs a binary classification for DNA sequences as sites for transcription binding. \n",
    "This multilayer-feed forward network utilizes backpropagation, a supervised learning method, used for classification and regression problems. Below are the following steps to the design of the ANN:\n",
    "1. Intialized network\n",
    "2. Foward Propagation\n",
    "3. Back Porpagate\n",
    "4. Train Network\n",
    "5. Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from Parse_seq import *\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(formatter={\"float\": \"{: 0.3f}\".format}, linewidth=np.inf)\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(z, derivative=False):\n",
    "    if derivative:\n",
    "        z = sigmoid(z)\n",
    "        return z * (1 - z)\n",
    "    z = np.clip(z, -500, 500)  # avoid overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, shape, activation=None):\n",
    "        self.num_layers = len(shape) - 1  # the input layer does not count as a layer\n",
    "        self.weight = []\n",
    "        self.bias = []\n",
    "\n",
    "        if activation is None:  # activation tuple is optional\n",
    "            self.activation = [sigmoid for _ in range(self.num_layers)]\n",
    "        else:\n",
    "            if len(shape) != len(activation):\n",
    "                raise ValueError(\"nr of layers ({}) must match activations ({})\".format(len(shape), len(activation)))\n",
    "            self.activation = activation[1:]  # skip the activation function for the input layer\n",
    "\n",
    "        self.a = []  # layers output after activation, input to the next layer\n",
    "        self.z = []  # layers results before activation\n",
    "        self.dw = []  # empty array used for weight update during training\n",
    "        self.db = []  # empty array used for bias update during training\n",
    "\n",
    "        for (layer1, layer2) in zip(shape[:-1], shape[1:]):\n",
    "            self.weight.append(np.random.normal(size=(layer2, layer1)))\n",
    "            self.bias.append(np.random.normal(size=(layer2, 1)))\n",
    "            self.dw.append(np.zeros((layer2, layer1)))\n",
    "            self.db.append(np.zeros((layer2, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.a = [x.T]  # a[0] is the input for layer 1 (layer 0 is the input layer)\n",
    "        self.z = [None]\n",
    "\n",
    "        for (weight, bias, activation) in zip(self.weight, self.bias, self.activation):\n",
    "            self.z.append(weight.dot(self.a[-1]) + bias)\n",
    "            self.a.append(activation(self.z[-1]))\n",
    "\n",
    "        return self.a[-1].T\n",
    "\n",
    "    def back_propagation(self, x, y, learning_rate=0.1, momentum=0.5):\n",
    "        m = x.shape[0]\n",
    "        delta_w = []\n",
    "        delta_b = []\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        #error = np.sum((y_hat - y) ** 2)\n",
    "        error = np.sqrt(np.sum((y_hat - y) ** 2))\n",
    "\n",
    "        for index in reversed(range(1, self.num_layers + 1)):\n",
    "            if index == self.num_layers:\n",
    "                da = self.a[index] - y.T\n",
    "            else:\n",
    "                da = self.weight[index].T.dot(dz)\n",
    "            dz = da * self.activation[index - 1](self.z[index], derivative=True)\n",
    "            dw = dz.dot(self.a[index - 1].T) / m\n",
    "            db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "\n",
    "            delta_w.append(dw)\n",
    "            delta_b.append(db)\n",
    "\n",
    "        for (index, dw, db) in zip(reversed(range(self.num_layers)), delta_w, delta_b):\n",
    "            self.dw[index] = learning_rate * dw + momentum * self.dw[index]\n",
    "            self.weight[index] -= self.dw[index]\n",
    "            self.db[index] = learning_rate * db + momentum * self.db[index]\n",
    "            self.bias[index] -= self.db[index]\n",
    "\n",
    "        return error\n",
    "    \n",
    "    def k_folds(self, sequences,expected, lr, n_hidden,k=5):\n",
    "        \"\"\"\n",
    "        Perform cross validation using the k folds approach where data\n",
    "        is split into training and testing sets k times using 100-k/k split\n",
    "        \"\"\"\n",
    "        accuracy, auc, tpr, fpr = [], [], [], []\n",
    "        # Split data\n",
    "        skf = StratifiedKFold(n_splits=k)\n",
    "        for train_ind, test_ind in skf.split(sequences, expected):\n",
    "            train_seq, test_seq = sequences[train_ind], sequences[test_ind]\n",
    "            train_exp, test_exp = expected[train_ind], expected[test_ind]\n",
    "            # train and test model\n",
    "            predictions = train_and_test(train_seq, train_exp, test_seq, n_hidden, lr)\n",
    "            # Caclulate true positive and false positive rate\n",
    "            fpr_roc, tpr_roc, thresholds = metrics.roc_curve(test_exp, predictions, pos_label=1)\n",
    "            auc.append(metrics.auc(fpr_roc, tpr_roc))\n",
    "            ACC, TPR, FPR = calc_static_sum_stats(test_exp, predictions)\n",
    "            # Add summary stats across splits\n",
    "            accuracy.append(ACC)\n",
    "            tpr.append(TPR) \n",
    "            fpr.append(FPR)\n",
    "\n",
    "        return np.average(accuracy), np.average(auc), tpr, fpr \n",
    "\n",
    "    def cross_validate(self, pos_seqs, neg_seqs, outfile, iters=10000):\n",
    "        \"\"\"\n",
    "        Perform cross validation for a range of learning rates and hidden layers.\n",
    "        Use kfold cross validation and output average accuracy\n",
    "        \"\"\"\n",
    "        train_seq, train_exp, test_seq, test_exp = preprocess(pos_seqs, neg_seqs, split=1)\n",
    "        with open(outfile, \"w\") as results:\n",
    "            results.write(\"hidden_layers\\tlearn_rate\\tavg_accuracy\\tavg_AUC\\tTPR\\tFPR\\n\")\n",
    "            for num_hidden in range(1,30, 4):\n",
    "                print(num_hidden)\n",
    "                for learn_rate in np.arange(0.01, 0.5, 0.05):\n",
    "                    print(learn_rate)\n",
    "                    avg_accuracy, avg_auc, tprs, fprs = self.k_folds(train_seq, train_exp, learn_rate, num_hidden)\n",
    "                    results.write(\"\\t\".join([str(num_hidden), str(learn_rate), str(avg_accuracy),\n",
    "                    str(avg_auc), \" \".join(tprs), \" \".join(fprs)]))\n",
    "                    results.write(\"\\n\")\n",
    "\n",
    "    def plot_roc(predictions, exp_test, outfile):\n",
    "        \"\"\"\n",
    "        plot ROC curve for predictions and expected values to asses model performance\n",
    "        \"\"\"\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(exp_test, predictions, pos_label=1)\n",
    "        AUC = metrics.auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Training Data (80/20) Split with AUC: \" + str(AUC))\n",
    "        plt.savefig(outfile, format=\"pdf\")\n",
    "        plt.close()\n",
    "        return\n",
    "\n",
    "    def train(self, x, y, iterations=10000, learning_rate=0.2, momentum=0.5, verbose=True):\n",
    "        min_error = 1e-5\n",
    "        loss = []\n",
    "\n",
    "        for i in range(iterations + 1):\n",
    "            error = self.back_propagation(x, y, learning_rate=learning_rate, momentum=momentum)\n",
    "            loss.append(error)\n",
    "            if verbose:\n",
    "                if i % 2500 == 0:\n",
    "                    print(\"iteration {:5d} error: {:0.6f}\".format(i, error))\n",
    "                if error <= min_error:\n",
    "                    print(\"minimum error {} reached at iteration {}\".format(min_error, i))\n",
    "                    break\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder:\n",
    "    Below I contructed a 8X8 identity matrix with an autoencoder with 3 hidden neurons. The multilayer-feed forward network constructed with a 8X8 autoencoder with 3 hidden neurons, and a sigmoid activation fuction.The input and output layers have 8 neurons that include a bias term connected to adajent layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology: Network Architecture\n",
    "    The function named NN() that creates a new neural network ready for training. It accepts three parameters, the number of inputs, the number of neurons to have in the hidden layer and the number of outputs.\n",
    "For the hidden layer, create n_hidden neurons and each neuron in the hidden layer has n_inputs + 1 weights, one for each input column in a dataset and an additional one for the bias.You can also see that the output layer that connects to the hidden layer has n_outputs neurons, each with n_hidden + 1 weights. This means that each neuron in the output layer connects to (has a weight for) each neuron in the hidden layer.We can calculate an output from a neural network by propagating an input signal through each layer until the output layer outputs its values.\n",
    "\n",
    "This is called forward-propagation. It is the technique needed to generate predictions during training that will need to be corrected, and it is the method we will need after the network is trained to make predictions on new data. The first step is to calculate the activation of one neuron given an input.The input could be a row from our training dataset, as in the case of the hidden layer. It may also be the outputs from each neuron in the hidden layer, in the case of the output layer. Neuron activation is calculated as the weighted sum of the inputs. Next to transfer the activation to see what the neuron output is this method uses sigmoid activation. Next we foward propagate the input. The function forward() implements the forward propagation for a row of data from our dataset with our neural network. The neuron’s output value is stored in the neuron with the name ‘z‘, 'a'. \n",
    "The function returns the outputs from the last layer also called the output layer.\n",
    "The backpropagation algorithm is named for the way in which weights are trained. Error is calculated between the expected outputs and the outputs forward propagated from the network. These errors are then propagated backward through the network from the output layer to the hidden layer, assigning blame for error and updating weights. To calculate transfer derivative with the given output value from a neuron, the slope is caluculated using the sigmoid transfer function. In error backprogation the first step is to calculate the error for each output neuron, this will give the error signal (input) to propagate backwards through the network. The error signal calculated for each neuron  and is stored. For training the model the function train() is used. The errors calulated for each neuron is used to update the weights. Weight, learning_rate and number of iterations can be modified to update the wieghts. Gradient descent was used to update the model. This involves first looping for a fixed number of epochs and within each epoch updating the network for each row in the training dataset. Because updates are made for each training pattern. If errors were accumulated across an epoch before updating the weights, this is called batch learning or batch gradient descent. The function that implements the training of an already initialized neural network with a given training dataset, learning rate, fixed number of epochs and an expected number of output values.\n",
    "The expected number of output values is used to transform class values in the training data into a one hot encoding. That is a binary vector with one column for each class value to match the output of the network. This is required to calculate the error for the output layer. The sum squared error between the expected output and the network output is accumulated each epoch and printed. This is helpful to create a trace of how much the network is learning and improving each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration     0 error: 4.446712\n",
      "iteration  2500 error: 1.704280\n",
      "iteration  5000 error: 1.159054\n",
      "iteration  7500 error: 1.007614\n",
      "iteration 10000 error: 0.805692\n",
      "iteration 12500 error: 0.605414\n",
      "iteration 15000 error: 0.502251\n",
      "iteration 17500 error: 0.437484\n",
      "iteration 20000 error: 0.391945\n",
      "iteration 22500 error: 0.357703\n",
      "iteration 25000 error: 0.330779\n",
      "iteration 27500 error: 0.308916\n",
      "iteration 30000 error: 0.290724\n",
      "iteration 32500 error: 0.275292\n",
      "iteration 35000 error: 0.261996\n",
      "iteration 37500 error: 0.250391\n",
      "iteration 40000 error: 0.240152\n",
      "iteration 42500 error: 0.231035\n",
      "iteration 45000 error: 0.222851\n",
      "iteration 47500 error: 0.215453\n",
      "iteration 50000 error: 0.208725\n",
      "predict: [[ 0.961  0.000  0.000  0.013  0.023  0.000  0.026  0.022]\n",
      " [ 0.000  0.947  0.014  0.000  0.004  0.027  0.000  0.042]\n",
      " [ 0.000  0.023  0.955  0.000  0.035  0.028  0.009  0.000]\n",
      " [ 0.013  0.002  0.000  0.955  0.000  0.030  0.001  0.036]\n",
      " [ 0.015  0.003  0.028  0.000  0.961  0.000  0.000  0.025]\n",
      " [ 0.000  0.041  0.017  0.033  0.000  0.951  0.009  0.000]\n",
      " [ 0.040  0.000  0.026  0.005  0.000  0.036  0.948  0.000]\n",
      " [ 0.034  0.039  0.000  0.034  0.032  0.000  0.000  0.925]]\n",
      "desired: [[ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]\n",
      " [ 0.000  1.000  0.000  0.000  0.000  0.000  0.000  0.000]\n",
      " [ 0.000  0.000  1.000  0.000  0.000  0.000  0.000  0.000]\n",
      " [ 0.000  0.000  0.000  1.000  0.000  0.000  0.000  0.000]\n",
      " [ 0.000  0.000  0.000  0.000  1.000  0.000  0.000  0.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000  1.000  0.000  0.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000  0.000  1.000  0.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000  0.000  0.000  1.000]]\n",
      "loss   : [[ 0.039 -0.000 -0.000 -0.013 -0.023 -0.000 -0.026 -0.022]\n",
      " [-0.000  0.053 -0.014 -0.000 -0.004 -0.027 -0.000 -0.042]\n",
      " [-0.000 -0.023  0.045 -0.000 -0.035 -0.028 -0.009 -0.000]\n",
      " [-0.013 -0.002 -0.000  0.045 -0.000 -0.030 -0.001 -0.036]\n",
      " [-0.015 -0.003 -0.028 -0.000  0.039 -0.000 -0.000 -0.025]\n",
      " [-0.000 -0.041 -0.017 -0.033 -0.000  0.049 -0.009 -0.000]\n",
      " [-0.040 -0.000 -0.026 -0.005 -0.000 -0.036  0.052 -0.000]\n",
      " [-0.034 -0.039 -0.000 -0.034 -0.032 -0.000 -0.000  0.075]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Autoencoder \n",
    "X = np.identity(8)\n",
    "Y = np.identity(8)\n",
    "\n",
    "network = NN((8, 3, 8), (None, sigmoid, sigmoid))\n",
    "\n",
    "error = network.train(X, Y, 50000, learning_rate=0.2)\n",
    "\n",
    "plt.plot(error)\n",
    "plt.xlabel(\"training iterations\")\n",
    "plt.ylabel(\"mse\")\n",
    "\n",
    "Y_hat = network.forward(X)\n",
    "\n",
    "print(\"predict:\", Y_hat.T)\n",
    "print(\"desired:\", Y.T)\n",
    "print(\"loss   :\", (Y - Y_hat).T)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Autoencoder Results\n",
    "\n",
    "The neural network designed a 8 dimension input 3 dimension hidden layer, 8 dimension output. I was not apply to apply a test that finds the best training parameters. Therefore I set the number of epochs to 5000 and the learning rate to 0.02. The model does well with prediction as the number of training iteration increases the mse decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Data preprocessing \n",
    "\n",
    "The functions called below are apart of another script file called Parse_seq: This is my data processing jupyter notebook. \n",
    "Rap1 Binding site prediction\n",
    "Postive sequneces: \n",
    "    137 sequences from rap1-leib-positive.txt were read into memory as string object.Sequences were encoded using a a one-hot encoding that looks like this:  'A':[1,0,0,0], 'C':[0,1,0,0], 'G':[0,0,1,0], 'T':[0,0,0,1]\n",
    "    This mapping from each sequence to a 4X17 matrix encoding sequnece.\n",
    " Negative sequences: All sequences from yeast-upstream-1k-negative.fa were read into memory\n",
    " \n",
    "Function called trim_neg_sequences()\n",
    "In the data preprocessing I decided to remove negative sequences that have postive sub-sequences by inputing the positive sequences and negative sequences, setting, at random, the length of bases to keep to equal 17 base pairs from 1K sequences, and ratio for number of negative sequences to positive sequences.The expected output should be negative sequences that are not similar to positive sequences and are the same number of base pairs as positive sequences.\n",
    "\n",
    "Represent DNA sequence: \n",
    "positive and negative sequences were encoded using a a one-hot encoding that looks like this:  'A':[1,0,0,0], 'C':[0,1,0,0], 'G':[0,0,1,0], 'T':[0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_seq, neg_sequences, test_seq, = read_seqs(pos_seq_file, neg_seq_file, test_seq_file)\n",
    "trim = trim_neg_sequences(pos_seq, neg_sequences, bp=17, ratio=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.pos (109,)\n",
      "(109, 68, 1)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# One hot encode postive and negative sequences\n",
    "pos_seq, neg_sequences, test_seq, = read_seqs(pos_seq_file, neg_seq_file, test_seq_file)\n",
    "trim = trim_neg_sequences(pos_seq, neg_sequences, bp=17, ratio=4)\n",
    "#Check shape: Training \n",
    "#one_hot_neg_list\n",
    "one_hot_neg_list = np.array(one_hot_neg(trim))\n",
    "one_hot_neg_list[0].shape\n",
    "\n",
    "one_hot_pos_list = np.array(one_hot_pos(pos_seq))\n",
    "one_hot_pos(pos_seq)[0].shape\n",
    "\n",
    "\n",
    "#Spilt positive sequences into training and testing sets\n",
    "data_split_train = split_data(one_hot_pos_list,one_hot_neg_list, split = .8)\n",
    "#data_split_train[0].shape\n",
    "\n",
    "#test_pos =  set(range(0,len(pos_seq)))- set(train_pos)\n",
    "#test_neg = set(range(0,len(trim)))- set(train_neg)\n",
    "#data_split_test = split_data(one_hot_test_list_p,one_hot_test_list_n, split = .3)\n",
    "#combine,labels = combine_and_shuffle(data_split[0],data_split[1])\n",
    "\n",
    "dataset=np.concatenate(data_split_train).reshape(218,68)\n",
    "Y = np.concatenate([np.repeat(1,109),np.repeat(0,109)])\n",
    "print(Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration     0 error: 122.953891\n",
      "iteration  2500 error: 146.717302\n",
      "iteration  5000 error: 150.098419\n",
      "iteration  7500 error: 151.180814\n",
      "iteration 10000 error: 151.699517\n",
      "iteration 12500 error: 152.016883\n",
      "iteration 15000 error: 152.240318\n",
      "iteration 17500 error: 152.411029\n",
      "iteration 20000 error: 152.548170\n",
      "iteration 22500 error: 152.661791\n",
      "iteration 25000 error: 152.757694\n",
      "iteration 27500 error: 152.839596\n",
      "iteration 30000 error: 152.910160\n",
      "iteration 32500 error: 152.971450\n",
      "iteration 35000 error: 153.025119\n",
      "iteration 37500 error: 153.072497\n",
      "iteration 40000 error: 153.114656\n",
      "iteration 42500 error: 153.152449\n",
      "iteration 45000 error: 153.186563\n",
      "iteration 47500 error: 153.217547\n",
      "iteration 50000 error: 153.245847\n"
     ]
    }
   ],
   "source": [
    "#Training the model:\n",
    "\n",
    "network = NN((68, 3, 1), (None, sigmoid, sigmoid))\n",
    "\n",
    "error = network.train(dataset, Y, 50000, learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: [[ 0.994  0.987  0.995  0.993  0.992  0.993  0.995  0.995  0.994  0.994  0.992  0.995  0.995  0.995  0.994  0.985  0.994  0.989  0.984  0.994  0.990  0.995  0.995  0.995  0.995  0.976  0.993  0.994  0.993  0.994  0.989  0.984  0.995  0.992  0.995  0.994  0.995  0.987  0.994  0.992  0.987  0.995  0.976  0.995  0.986  0.994  0.985  0.993  0.995  0.994  0.994  0.992  0.994  0.995  0.995  0.995  0.995  0.995  0.988  0.994  0.986  0.995  0.995  0.988  0.993  0.993  0.976  0.994  0.990  0.994  0.995  0.995  0.995  0.991  0.965  0.994  0.984  0.980  0.995  0.988  0.994  0.993  0.990  0.994  0.995  0.993  0.995  0.994  0.994  0.986  0.995  0.979  0.994  0.977  0.994  0.995  0.995  0.995  0.988  0.994  0.994  0.991  0.994  0.995  0.994  0.994  0.995  0.995  0.995  0.004  0.002  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.002  0.001  0.001  0.001  0.003  0.003  0.003  0.003  0.003  0.001  0.002  0.001  0.005  0.001  0.013  0.001  0.002  0.005  0.001  0.001  0.003  0.002  0.001  0.002  0.002  0.003  0.004  0.003  0.002  0.001  0.001  0.003  0.002  0.001  0.019  0.001  0.001  0.001  0.003  0.008  0.001  0.001  0.003  0.003  0.001  0.003  0.001  0.001  0.001  0.012  0.005  0.002  0.001  0.009  0.001  0.001  0.001  0.001  0.001  0.134  0.001  0.022  0.001  0.002  0.001  0.001  0.001  0.001  0.005  0.003  0.001  0.001  0.006  0.001  0.003  0.002  0.003  0.002  0.003  0.003  0.002  0.001  0.001  0.003  0.003  0.003  0.001  0.002  0.001  0.001  0.001  0.003  0.003  0.003]]\n",
      "desired: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "loss   : 13034.409162463\n"
     ]
    }
   ],
   "source": [
    "Y_hat = network.forward(dataset)\n",
    "\n",
    "print(\"predict:\", Y_hat.T)\n",
    "print(\"desired:\", Y.T)\n",
    "print(\"loss   :\", np.sum((Y - Y_hat).T)**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second testing of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration     0 error: 139.259633\n",
      "iteration  2500 error: 141.809978\n",
      "iteration  5000 error: 146.786227\n",
      "iteration  7500 error: 148.620497\n",
      "iteration 10000 error: 149.498661\n",
      "iteration 12500 error: 150.078363\n",
      "iteration 15000 error: 150.503072\n",
      "iteration 17500 error: 150.831909\n",
      "iteration 20000 error: 151.095455\n",
      "iteration 22500 error: 151.312098\n",
      "iteration 25000 error: 151.494027\n",
      "iteration 27500 error: 151.649714\n",
      "iteration 30000 error: 151.785121\n",
      "iteration 32500 error: 151.904394\n",
      "iteration 35000 error: 152.010359\n",
      "iteration 37500 error: 152.104957\n",
      "iteration 40000 error: 152.189632\n",
      "iteration 42500 error: 152.265588\n",
      "iteration 45000 error: 152.333915\n",
      "iteration 47500 error: 152.395610\n",
      "iteration 50000 error: 152.451568\n",
      "iteration 52500 error: 152.502568\n",
      "iteration 55000 error: 152.549275\n",
      "iteration 57500 error: 152.592249\n",
      "iteration 60000 error: 152.631959\n"
     ]
    }
   ],
   "source": [
    "network = NN((68, 3, 1), (None, sigmoid, sigmoid))\n",
    "\n",
    "error = network.train(dataset, Y, 60000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: [[ 0.988  0.987  0.987  0.988  0.987  0.988  0.983  0.986  0.987  0.986  0.986  0.987  0.986  0.984  0.987  0.980  0.987  0.986  0.985  0.988  0.987  0.988  0.986  0.987  0.986  0.983  0.986  0.986  0.988  0.980  0.985  0.983  0.987  0.986  0.985  0.987  0.987  0.965  0.974  0.988  0.985  0.988  0.974  0.988  0.977  0.982  0.987  0.981  0.985  0.987  0.985  0.986  0.977  0.986  0.987  0.986  0.987  0.987  0.986  0.973  0.985  0.987  0.988  0.986  0.979  0.984  0.986  0.988  0.987  0.987  0.982  0.987  0.987  0.987  0.985  0.986  0.987  0.987  0.987  0.986  0.988  0.984  0.988  0.983  0.987  0.984  0.987  0.979  0.987  0.982  0.987  0.985  0.987  0.987  0.984  0.988  0.988  0.975  0.983  0.985  0.986  0.987  0.980  0.977  0.986  0.985  0.983  0.987  0.988  0.000  0.000  0.000  0.000  0.000  0.000  0.003  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.005  0.028  0.000  0.000  0.000  0.000  0.001  0.001  0.000  0.001  0.000  0.031  0.001  0.000  0.000  0.007  0.000  0.000  0.000  0.000  0.000  0.113  0.007  0.000  0.000  0.000  0.001  0.004  0.000  0.001  0.000  0.000  0.000  0.000  0.002  0.001  0.000  0.000  0.032  0.000  0.009  0.000  0.000  0.000  0.003  0.000  0.000  0.000  0.018  0.000  0.000  0.002  0.000  0.000  0.036  0.013  0.012  0.000  0.001  0.000  0.000  0.000  0.000  0.071  0.000  0.000  0.000  0.032  0.017  0.000  0.000  0.000  0.000  0.001  0.010  0.000  0.015  0.000  0.000  0.037  0.000  0.001  0.000  0.000  0.000  0.000  0.000  0.000  0.014]]\n",
      "desired: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "loss   : 57793.59577407668\n"
     ]
    }
   ],
   "source": [
    "Y_hat = network.forward(dataset)\n",
    "\n",
    "print(\"predict:\", Y_hat.T)\n",
    "print(\"desired:\", Y.T)\n",
    "print(\"loss   :\", np.sum((Y - Y_hat).T)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of this code is above and below. I couldn't get this to work so I tried different processes. \n",
    "\n",
    "Develop a training regime (K-fold cross validation, bagging, etc) to test model performance.\n",
    "In the NN function I tried to apply a K-fold validation by separating the positives into k groups, separate negatives into k groups, take 1 group from pos, take 1 group from neg, concat those : test set. concat remaining pos groups, concat remaining neg groups, concat remaining pos with remaining neg : training set, repeat k-1 more times, holding out a different group each time. It would be nice to have perform the cross validation for a range of learning rates and hidden layers. Use kfold cross validation and output average accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(train_in, train_out, test_in,\n",
    "\t\tnum_hidden, learn_rate, func=\"log\", iters=10000):\n",
    "\t\"\"\"\n",
    "\tTrain and test method simultaneously and output predictions\n",
    "\tINPUT: training data, expected outputs for training, and testing data\n",
    "\tnumber of hidden nodes and learning rate\n",
    "\tOUTPUT: predictions\n",
    "\t\"\"\"\n",
    "\t# Train model\n",
    "\trap1_nnet = NN(train_in, num_hidden, 1)\n",
    "\trap1_nnet.func = func\n",
    "\trap1_nnet.train(train_in, train_out, iters, lr=learn_rate)\n",
    "\n",
    "\t# make predictions\n",
    "\tpredictions = rap1_nnet.test(test_in)\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3be6f460e855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-211a07d4904e>\u001b[0m in \u001b[0;36mk_folds\u001b[0;34m(self, sequences, expected, lr, n_hidden, k)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mtrain_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# train and test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Caclulate true positive and false positive rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mfpr_roc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_roc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-7a157a1b6717>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(train_in, train_out, test_in, num_hidden, learn_rate, func, iters)\u001b[0m\n\u001b[1;32m      8\u001b[0m \t\"\"\"\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mrap1_nnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mrap1_nnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrap1_nnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    " folds = network.k_folds(dataset,Y, lr=.2, n_hidden=3,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model: \n",
    " How was your training regime designed so as to prevent the negative training data from overwhelming the positive training data? \n",
    " I implemeted a helper functions that randomly divides and both postive and negative datasets in to training and test set cases of equal size. Balance sets. \n",
    "\n",
    "What was your stop criterion for convergence in your learned parameters? How did you decide this? \n",
    "\n",
    "The stop criterion  was set to number of epochs = 50000, and the learning rate = 0.02. I would have love to decided this by testing out a range of stop criteron on the cross validation scheme. Doing this would have allowed me to evaluate how sensitve these parameters are using 5 to 10 folds. I would suspect that as the learning_rate decreases and number of iterations increases are better parameters for convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question number 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross-validation experiments to test model hyperparameters. (1 point)\n",
    "Develop and describe your choice of model hyperparameters \n",
    "Answer question 4 questions\n",
    " Describe how you set up your experiment to measure your system's performance. \n",
    " To measure the models performace I decided to implement ROC/AUC getting using the prediction and expexted test datasets. I hope to get a, tpr fpr,True_positive ratio  and false positve ratios. I strive to balance the datasets to minimize the bias toward the dominate trends, negative sequences v. positive, in the training datasets. If the training sets were unbalanced this would affect the performance of the NN model. Using the k-fold method it allows the NN to dee train on the datasets more allowing the NN model to perform better. \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "- What set of learning parameters works the best? Please provide sample output from your system. \n",
    "learning parameter of 0.02 works well. \n",
    "\n",
    "- What are the effects of altering your system (e.g. number of hidden units or choice of kernel function)? Why do you think you observe these effects? \n",
    "I suspect there are no effects to altering my system if I increase or decrease the number of hidden layers. I do suspect that if I alter the number of input and output lyaer my NN would not even initiate. \n",
    "- What other parameters, if any, affect performance? \n",
    "epochs,the number of folds in the cross validation, gradient descienct I could have use stochastic gradient descent. learning rate , and momemtem can affect performance. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.pos (41,)\n",
      "(41, 68, 1)\n",
      "test_pos 96\n",
      "test_neg 507\n"
     ]
    }
   ],
   "source": [
    "split= 0.3\n",
    "pos_size, neg_size = int(len(one_hot_pos_list)*split), int(len(one_hot_neg_list)*split)\n",
    "train_pos = np.random.choice(range(0,pos_size), size=pos_size , replace=False)\n",
    "print('train.pos',train_pos.shape)\n",
    "train_neg = np.random.choice(range(0,neg_size), size=pos_size,  replace=False)\n",
    "print(one_hot_pos_list[train_pos].shape)\n",
    "test_pos =  set(range(0,len(one_hot_pos_list)))- set(train_pos)\n",
    "test_neg = set(range(0,len(one_hot_neg_list)))- set(train_neg)\n",
    "print('test_pos',len(test_pos))\n",
    "print('test_neg',len(test_neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_pos_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-94e8ed124b6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Read in sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpos_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_seqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pos_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_neg_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Select negative data randomly with ratio to positive seqs, select 17bp region,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mneg_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_neg_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_pos_file' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "######################## K-FOLDS CROSS VALIDATION FOR PARAMETER TUNING ##############################################\n",
    "if len(sys.argv) > 4 and sys.argv[4] == \"--optimize\":\n",
    "\t# Loop through ratios of positive to negative sequences and perform cross validation to find \n",
    "\t# best number of hidden layers and the best learning rate\n",
    "\tfor r in [5,10]:\n",
    "\t\t# Read in sequences\n",
    "\t\tpos_seq, neg_sequences, test_seq = read_seqs(train_pos_file, train_neg_file, test_seq_file)\n",
    "\t\t# Filter negative sequenes to avoid class imbalance\n",
    "\t\tprint(\"Number of RAP1 binding sequences: \"+str(len(pos_seq)))\n",
    "\t\tprint(\"Number of non-binding sequences: \"+str(len(neg_seq)))\n",
    "\t\tneg_seq = trim_neg_sequences(pos_seq, neg_seq, ratio = r)\n",
    "\t\tprint(\"Number of non-binding sequences after filtering and downsampling using ratio \"+ str(r)+ \": \"+str(len(neg_seq)))\n",
    "\t\tprint(\"\\n\")\n",
    "\n",
    "\t\t# Cross validation\n",
    "\t\tcross_validate(pos_seq, neg_seq, \"cross_val_summary_log_ratio\"+str(r)+\".txt\")\n",
    "\n",
    "else:\n",
    "\t# Read in sequences\n",
    "\tpos_seq, neg_sequences, unknown_seq = read_seqs(train_pos_file, train_neg_file, test_seq_file)\n",
    "\t# Select negative data randomly with ratio to positive seqs, select 17bp region,\n",
    "\tneg_seq = trim_neg_sequences(pos_seq, neg_sequences, ratio = 5)\n",
    "\t# Shuffle and encode training and validation data\n",
    "\ttrain_seq, train_exp, test_seq, test_exp = preprocess(pos_seq, neg_sequences, split=1)\n",
    "\tunknown_seq_encode = np.array([encode_DNA(seq) for seq in unknown_seq])\n",
    "\t# Use previously found validated parameters for training\n",
    "\trap1_nnet = NN(train_seq.shape[1],25,1)\n",
    "\tmse = rap1_nnet.train(train_seq, train_exp, iterations=20000, learning_rate=0.01)\n",
    "\tprint(len(mse))\n",
    "\tplot_MSE(mse, \"MSE_RAP1_binding.pdf\", \"Error Minimization in RAP1 Binding Site Problem\")\n",
    "\t# Test data and output predictions\n",
    "\tout = open(\"/Users/tiannagrant/Downloads/Final_Project_Skeleton-master/predictions.txt\", \"w\")\n",
    "\tpredictions = rap1_nnet.test(unknown_seq_encode)\n",
    "\tfor p in range(0, len(predictions)):\n",
    "\t\tout.write(unknown_seq[p] + \"\\t\" + str(predictions[p]))\n",
    "\t\tout.write(\"\\n\")\n",
    "\tout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
